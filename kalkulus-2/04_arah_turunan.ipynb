{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 - Arah Turunan dan Gradien\n",
        "## Directional Derivatives and Gradient\n",
        "\n",
        "### Deskripsi\n",
        "Notebook ini membahas turunan arah, gradien, dan aplikasinya dalam optimasi dan analisis medan vektor.\n",
        "\n",
        "### Learning Objectives\n",
        "- Memahami konsep turunan arah\n",
        "- Menghitung turunan arah menggunakan gradien\n",
        "- Memahami interpretasi geometris gradien\n",
        "- Menerapkan dalam optimasi\n",
        "- Visualisasi medan gradien\n",
        "\n",
        "### Prerequisites\n",
        "- Turunan parsial\n",
        "- Dasar-dasar vektor\n",
        "- Python dasar dan matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import sympy as sp\n",
        "from sympy import symbols, diff, simplify, sqrt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Ready to explore directional derivatives and gradients!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Turunan Arah (Directional Derivative)\n",
        "\n",
        "### Definisi\n",
        "Turunan arah dari fungsi $f(x,y)$ dalam arah vektor satuan $\\hat{u} = (a,b)$ adalah:\n",
        "\n",
        "$$D_{\\hat{u}}f(x,y) = \\nabla f \\cdot \\hat{u} = \\frac{\\partial f}{\\partial x}a + \\frac{\\partial f}{\\partial y}b$$\n",
        "\n",
        "### Interpretasi Geometris\n",
        "- Turunan arah menunjukkan laju perubahan fungsi dalam arah tertentu\n",
        "- Nilai maksimum terjadi ketika $\\hat{u}$ searah dengan gradien\n",
        "- Nilai minimum terjadi ketika $\\hat{u}$ berlawanan arah dengan gradien\n",
        "- Besar turunan arah maksimum sama dengan besar gradien\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contoh turunan arah\n",
        "def directional_derivative_example():\n",
        "    \"\"\"Menghitung turunan arah untuk f(x,y) = x² + y²\"\"\"\n",
        "    x, y, a, b = symbols('x y a b')\n",
        "    \n",
        "    # Fungsi\n",
        "    f = x**2 + y**2\n",
        "    \n",
        "    # Gradien\n",
        "    fx = diff(f, x)\n",
        "    fy = diff(f, y)\n",
        "    \n",
        "    # Vektor arah (normalized)\n",
        "    # u = (a, b) dengan |u| = 1, jadi a² + b² = 1\n",
        "    \n",
        "    # Turunan arah\n",
        "    D_u_f = fx*a + fy*b\n",
        "    \n",
        "    print(\"Turunan Arah:\")\n",
        "    print(\"=\" * 20)\n",
        "    print(f\"f(x,y) = {f}\")\n",
        "    print(f\"∇f = ({fx}, {fy})\")\n",
        "    print(f\"u = (a, b) dengan a² + b² = 1\")\n",
        "    print(f\"\\nTurunan arah:\")\n",
        "    print(f\"D_u f = ∇f · u = {fx}·a + {fy}·b\")\n",
        "    print(f\"D_u f = {D_u_f}\")\n",
        "    \n",
        "    # Contoh spesifik: arah (1, 0) dan (0, 1)\n",
        "    print(f\"\\nContoh spesifik:\")\n",
        "    print(f\"Arah (1, 0): D_u f = {D_u_f.subs([(a, 1), (b, 0)])} = 2x\")\n",
        "    print(f\"Arah (0, 1): D_u f = {D_u_f.subs([(a, 0), (b, 1)])} = 2y\")\n",
        "    \n",
        "    return D_u_f\n",
        "\n",
        "directional_deriv = directional_derivative_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Gradien dan Arah Kenaikan Tercepat\n",
        "\n",
        "### Teorema\n",
        "Turunan arah maksimum dari fungsi $f(x,y)$ di titik $(a,b)$ adalah:\n",
        "\n",
        "$$\\max D_{\\hat{u}}f(a,b) = |\\nabla f(a,b)|$$\n",
        "\n",
        "dan terjadi ketika $\\hat{u}$ searah dengan $\\nabla f(a,b)$.\n",
        "\n",
        "### Arah Kenaikan dan Penurunan Tercepat\n",
        "- **Arah kenaikan tercepat**: $\\hat{u} = \\frac{\\nabla f}{|\\nabla f|}$\n",
        "- **Arah penurunan tercepat**: $\\hat{u} = -\\frac{\\nabla f}{|\\nabla f|}$\n",
        "- **Arah tanpa perubahan**: $\\hat{u}$ tegak lurus terhadap $\\nabla f$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisasi gradien dan turunan arah\n",
        "def plot_gradient_direction():\n",
        "    \"\"\"Plot gradien dan arah kenaikan tercepat\"\"\"\n",
        "    # Create grid\n",
        "    x = np.linspace(-3, 3, 20)\n",
        "    y = np.linspace(-3, 3, 20)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    \n",
        "    # Function f(x,y) = x² + y²\n",
        "    Z = X**2 + Y**2\n",
        "    \n",
        "    # Gradient components\n",
        "    Fx = 2*X\n",
        "    Fy = 2*Y\n",
        "    \n",
        "    # Gradient magnitude\n",
        "    grad_mag = np.sqrt(Fx**2 + Fy**2)\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Plot 1: Gradient field\n",
        "    contour = ax1.contour(X, Y, Z, levels=10, colors='black', alpha=0.6)\n",
        "    ax1.clabel(contour, inline=True, fontsize=8)\n",
        "    ax1.quiver(X, Y, Fx, Fy, alpha=0.7, color='red')\n",
        "    ax1.set_title('Gradient Field of f(x,y) = x² + y²')\n",
        "    ax1.set_xlabel('x')\n",
        "    ax1.set_ylabel('y')\n",
        "    ax1.grid(True)\n",
        "    ax1.set_aspect('equal')\n",
        "    \n",
        "    # Plot 2: Gradient magnitude\n",
        "    im = ax2.contourf(X, Y, grad_mag, levels=20, cmap='viridis')\n",
        "    ax2.contour(X, Y, grad_mag, levels=20, colors='black', alpha=0.3)\n",
        "    ax2.quiver(X, Y, Fx, Fy, alpha=0.7, color='white')\n",
        "    ax2.set_title('Gradient Magnitude |∇f|')\n",
        "    ax2.set_xlabel('x')\n",
        "    ax2.set_ylabel('y')\n",
        "    ax2.set_aspect('equal')\n",
        "    plt.colorbar(im, ax=ax2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_gradient_direction()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Aplikasi dalam Optimasi\n",
        "\n",
        "### Gradient Descent\n",
        "Algoritma optimasi yang menggunakan gradien untuk mencari minimum:\n",
        "\n",
        "1. Mulai dari titik awal $x_0$\n",
        "2. Update: $x_{n+1} = x_n - \\alpha \\nabla f(x_n)$\n",
        "3. Ulangi sampai konvergen\n",
        "\n",
        "dimana $\\alpha$ adalah learning rate.\n",
        "\n",
        "### Aplikasi Real-World\n",
        "- **Machine Learning**: Optimasi parameter model\n",
        "- **Fisika**: Analisis medan potensial\n",
        "- **Engineering**: Desain optimal\n",
        "- **Ekonomi**: Optimasi utilitas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Ringkasan dan Kesimpulan\n",
        "\n",
        "### Konsep Utama yang Dipelajari\n",
        "1. **Turunan Arah**: Laju perubahan fungsi dalam arah tertentu\n",
        "2. **Gradien**: Vektor yang menunjukkan arah kenaikan tercepat\n",
        "3. **Arah Kenaikan Tercepat**: Searah dengan gradien\n",
        "4. **Arah Penurunan Tercepat**: Berlawanan arah dengan gradien\n",
        "5. **Gradient Descent**: Algoritma optimasi menggunakan gradien\n",
        "\n",
        "### Rumus Penting\n",
        "- **Turunan Arah**: $D_{\\hat{u}}f = \\nabla f \\cdot \\hat{u}$\n",
        "- **Gradien**: $\\nabla f = \\frac{\\partial f}{\\partial x}\\hat{i} + \\frac{\\partial f}{\\partial y}\\hat{j}$\n",
        "- **Turunan Arah Maksimum**: $|\\nabla f|$\n",
        "- **Arah Kenaikan Tercepat**: $\\hat{u} = \\frac{\\nabla f}{|\\nabla f|}$\n",
        "\n",
        "### Aplikasi Praktis\n",
        "- Optimasi dalam machine learning\n",
        "- Analisis medan vektor dalam fisika\n",
        "- Desain optimal dalam engineering\n",
        "- Optimasi ekonomi dan bisnis\n",
        "\n",
        "### Langkah Selanjutnya\n",
        "Materi selanjutnya akan membahas **Optimasi Multivariabel** yang merupakan aplikasi lanjutan dari turunan arah dan gradien.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
