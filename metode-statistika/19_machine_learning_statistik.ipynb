{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 19. Machine Learning Statistik (Statistical Machine Learning)\n",
        "\n",
        "## Tujuan Pembelajaran\n",
        "- Memahami hubungan antara statistika dan machine learning\n",
        "- Menerapkan konsep statistik dalam machine learning\n",
        "- Menggunakan validasi statistik untuk model ML\n",
        "- Memahami bias-variance tradeoff dan implikasinya\n",
        "- Menerapkan cross-validation dan bootstrap untuk validasi model\n",
        "- Memahami konsep overfitting dan underfitting\n",
        "- Menerapkan regularisasi untuk mengatasi overfitting\n",
        "- Memahami model selection dan evaluation metrics\n",
        "- Menerapkan statistical learning theory dalam praktik\n",
        "- Menggunakan confidence intervals dan hypothesis testing dalam ML\n",
        "- Memahami resampling methods dan their applications\n",
        "- Menerapkan ensemble methods dengan dasar statistik\n",
        "- Memahami feature selection dan dimensionality reduction\n",
        "- Menerapkan statistical validation dalam model comparison\n",
        "- Menggunakan statistical inference dalam machine learning\n",
        "\n",
        "## Materi\n",
        "1. **Pengantar Machine Learning Statistik (Statistical ML Introduction)**\n",
        "   - Hubungan statistika dan machine learning\n",
        "   - Statistical learning theory\n",
        "   - Supervised vs unsupervised learning\n",
        "   - Parametric vs non-parametric methods\n",
        "   - Model complexity dan generalization\n",
        "\n",
        "2. **Bias-Variance Tradeoff**\n",
        "   - Konsep bias dan variance\n",
        "   - Bias-variance decomposition\n",
        "   - Overfitting dan underfitting\n",
        "   - Model complexity vs performance\n",
        "   - Practical implications\n",
        "\n",
        "3. **Cross-Validation dan Bootstrap**\n",
        "   - K-fold cross-validation\n",
        "   - Leave-one-out cross-validation\n",
        "   - Bootstrap sampling\n",
        "   - Validation strategies\n",
        "   - Nested cross-validation\n",
        "\n",
        "4. **Regularization (Ridge, Lasso, Elastic Net)**\n",
        "   - Ridge regression (L2 regularization)\n",
        "   - Lasso regression (L1 regularization)\n",
        "   - Elastic Net regularization\n",
        "   - Regularization parameter tuning\n",
        "   - Feature selection dengan regularization\n",
        "\n",
        "5. **Model Selection dan Evaluation**\n",
        "   - Information criteria (AIC, BIC)\n",
        "   - Cross-validation metrics\n",
        "   - Model comparison techniques\n",
        "   - Statistical significance testing\n",
        "   - Multiple comparison correction\n",
        "\n",
        "6. **Resampling Methods**\n",
        "   - Bootstrap confidence intervals\n",
        "   - Permutation tests\n",
        "   - Jackknife estimation\n",
        "   - Monte Carlo methods\n",
        "   - Statistical significance testing\n",
        "\n",
        "7. **Ensemble Methods**\n",
        "   - Bagging (Bootstrap Aggregating)\n",
        "   - Random Forest\n",
        "   - Boosting methods\n",
        "   - Stacking\n",
        "   - Voting classifiers\n",
        "\n",
        "8. **Feature Selection dan Dimensionality Reduction**\n",
        "   - Filter methods\n",
        "   - Wrapper methods\n",
        "   - Embedded methods\n",
        "   - Principal Component Analysis (PCA)\n",
        "   - Feature importance\n",
        "\n",
        "9. **Statistical Validation dalam ML**\n",
        "   - Hypothesis testing untuk model comparison\n",
        "   - Confidence intervals untuk predictions\n",
        "   - Statistical significance dalam feature selection\n",
        "   - Multiple testing correction\n",
        "   - Power analysis\n",
        "\n",
        "10. **Aplikasi dalam Analisis Data**\n",
        "    - Medical diagnosis\n",
        "    - Financial modeling\n",
        "    - Marketing analytics\n",
        "    - Scientific research\n",
        "    - Business intelligence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold, LeaveOneOut, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from scipy import stats\n",
        "from scipy.stats import bootstrap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "print(\"Pandas version:\", pd.__version__)\n",
        "print(\"Scikit-learn version:\", __import__('sklearn').__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Pengantar Machine Learning Statistik (Statistical ML Introduction)\n",
        "\n",
        "### 1.1 Hubungan Statistika dan Machine Learning\n",
        "\n",
        "**Machine Learning Statistik** adalah bidang yang menggabungkan prinsip-prinsip statistika dengan teknik machine learning untuk membuat model prediktif yang robust dan dapat diinterpretasikan.\n",
        "\n",
        "#### 1.1.1 Perspektif Statistika\n",
        "- **Inference**: Membuat kesimpulan tentang populasi dari sampel\n",
        "- **Uncertainty Quantification**: Mengukur ketidakpastian dalam prediksi\n",
        "- **Hypothesis Testing**: Menguji signifikansi statistik\n",
        "- **Confidence Intervals**: Memberikan rentang kepercayaan untuk estimasi\n",
        "- **Model Validation**: Memvalidasi model dengan metode statistik\n",
        "\n",
        "#### 1.1.2 Perspektif Machine Learning\n",
        "- **Prediction**: Membuat prediksi yang akurat\n",
        "- **Pattern Recognition**: Mengenali pola dalam data\n",
        "- **Automation**: Mengotomatisasi proses pengambilan keputusan\n",
        "- **Scalability**: Menangani data dalam skala besar\n",
        "- **Feature Engineering**: Mengidentifikasi fitur yang relevan\n",
        "\n",
        "#### 1.1.3 Konvergensi Kedua Bidang\n",
        "- **Statistical Learning Theory**: Teori matematis untuk pembelajaran\n",
        "- **Empirical Risk Minimization**: Minimisasi risiko empiris\n",
        "- **Generalization**: Kemampuan model bekerja pada data baru\n",
        "- **Regularization**: Teknik untuk mencegah overfitting\n",
        "- **Cross-Validation**: Validasi model dengan data terbatas\n",
        "\n",
        "### 1.2 Statistical Learning Theory\n",
        "\n",
        "#### 1.2.1 Konsep Dasar\n",
        "**Statistical Learning Theory** memberikan fondasi matematis untuk machine learning dengan fokus pada:\n",
        "\n",
        "1. **Generalization Error**: Error pada data yang belum pernah dilihat\n",
        "2. **Sample Complexity**: Berapa banyak data yang dibutuhkan\n",
        "3. **Learning Rate**: Seberapa cepat model belajar\n",
        "4. **Convergence**: Konvergensi algoritma pembelajaran\n",
        "\n",
        "#### 1.2.2 Vapnik-Chervonenkis (VC) Theory\n",
        "- **VC Dimension**: Mengukur kapasitas model\n",
        "- **VC Bound**: Batas atas untuk generalization error\n",
        "- **Structural Risk Minimization**: Prinsip untuk memilih model\n",
        "- **Margin Theory**: Teori margin untuk support vector machines\n",
        "\n",
        "#### 1.2.3 Empirical Risk Minimization (ERM)\n",
        "```\n",
        "R(f) = E[L(f(X), Y)]  # True risk\n",
        "R_emp(f) = (1/n) Σ L(f(x_i), y_i)  # Empirical risk\n",
        "```\n",
        "\n",
        "Dimana:\n",
        "- R(f) = true risk (expected loss)\n",
        "- R_emp(f) = empirical risk (average loss on training data)\n",
        "- L = loss function\n",
        "- f = model function\n",
        "\n",
        "### 1.3 Supervised vs Unsupervised Learning\n",
        "\n",
        "#### 1.3.1 Supervised Learning\n",
        "**Tujuan**: Mempelajari mapping dari input ke output yang diketahui\n",
        "\n",
        "**Karakteristik**:\n",
        "- Data berlabel (X, y)\n",
        "- Tujuan prediksi atau klasifikasi\n",
        "- Evaluasi dengan ground truth\n",
        "- Contoh: regression, classification\n",
        "\n",
        "**Metode Statistik**:\n",
        "- **Regression**: Linear regression, polynomial regression\n",
        "- **Classification**: Logistic regression, Naive Bayes\n",
        "- **Validation**: Cross-validation, holdout validation\n",
        "- **Inference**: Confidence intervals, hypothesis testing\n",
        "\n",
        "#### 1.3.2 Unsupervised Learning\n",
        "**Tujuan**: Mempelajari struktur dalam data tanpa label\n",
        "\n",
        "**Karakteristik**:\n",
        "- Data tidak berlabel (X)\n",
        "- Tujuan eksplorasi dan discovery\n",
        "- Evaluasi dengan internal metrics\n",
        "- Contoh: clustering, dimensionality reduction\n",
        "\n",
        "**Metode Statistik**:\n",
        "- **Clustering**: K-means, hierarchical clustering\n",
        "- **Dimensionality Reduction**: PCA, ICA, t-SNE\n",
        "- **Density Estimation**: Kernel density estimation\n",
        "- **Anomaly Detection**: Statistical outlier detection\n",
        "\n",
        "### 1.4 Parametric vs Non-Parametric Methods\n",
        "\n",
        "#### 1.4.1 Parametric Methods\n",
        "**Karakteristik**:\n",
        "- Jumlah parameter tetap\n",
        "- Asumsi bentuk fungsi\n",
        "- Lebih efisien dengan data kecil\n",
        "- Interpretasi yang mudah\n",
        "\n",
        "**Contoh**:\n",
        "- **Linear Regression**: y = β₀ + β₁x₁ + ... + βₖxₖ + ε\n",
        "- **Logistic Regression**: P(y=1|x) = 1/(1 + e^(-βᵀx))\n",
        "- **Naive Bayes**: P(y|x) ∝ P(y) ∏ P(xᵢ|y)\n",
        "\n",
        "**Keuntungan**:\n",
        "- Interpretasi yang jelas\n",
        "- Confidence intervals untuk parameter\n",
        "- Hypothesis testing\n",
        "- Efisien dengan data kecil\n",
        "\n",
        "**Keterbatasan**:\n",
        "- Asumsi bentuk fungsi yang kaku\n",
        "- Mungkin tidak cocok untuk data kompleks\n",
        "- Sensitif terhadap outliers\n",
        "\n",
        "#### 1.4.2 Non-Parametric Methods\n",
        "**Karakteristik**:\n",
        "- Jumlah parameter bertambah dengan data\n",
        "- Tidak ada asumsi bentuk fungsi\n",
        "- Lebih fleksibel\n",
        "- Membutuhkan data lebih banyak\n",
        "\n",
        "**Contoh**:\n",
        "- **k-Nearest Neighbors**: Prediksi berdasarkan k tetangga terdekat\n",
        "- **Decision Trees**: Aturan if-then yang hierarkis\n",
        "- **Support Vector Machines**: Hyperplane optimal\n",
        "- **Neural Networks**: Komposisi fungsi non-linear\n",
        "\n",
        "**Keuntungan**:\n",
        "- Fleksibilitas tinggi\n",
        "- Dapat menangani data kompleks\n",
        "- Tidak memerlukan asumsi distribusi\n",
        "- Robust terhadap outliers\n",
        "\n",
        "**Keterbatasan**:\n",
        "- Interpretasi yang sulit\n",
        "- Membutuhkan data banyak\n",
        "- Overfitting risk tinggi\n",
        "- Computational cost tinggi\n",
        "\n",
        "### 1.5 Model Complexity dan Generalization\n",
        "\n",
        "#### 1.5.1 Model Complexity\n",
        "**Definisi**: Jumlah parameter atau kapasitas model untuk mempelajari data\n",
        "\n",
        "**Faktor yang Mempengaruhi**:\n",
        "- **Jumlah Parameter**: Lebih banyak parameter = lebih kompleks\n",
        "- **Model Flexibility**: Kemampuan menangani hubungan non-linear\n",
        "- **Feature Space**: Dimensi dan jenis fitur\n",
        "- **Regularization**: Teknik untuk mengontrol kompleksitas\n",
        "\n",
        "#### 1.5.2 Generalization\n",
        "**Definisi**: Kemampuan model bekerja pada data yang belum pernah dilihat\n",
        "\n",
        "**Faktor yang Mempengaruhi**:\n",
        "- **Training Data Size**: Lebih banyak data = generalisasi lebih baik\n",
        "- **Model Complexity**: Kompleksitas optimal untuk data\n",
        "- **Noise Level**: Tingkat noise dalam data\n",
        "- **Feature Quality**: Relevansi dan kualitas fitur\n",
        "\n",
        "#### 1.5.3 Bias-Variance Tradeoff\n",
        "```\n",
        "Expected Error = Bias² + Variance + Noise\n",
        "```\n",
        "\n",
        "**Bias**: Error karena asumsi model yang terlalu sederhana\n",
        "**Variance**: Error karena sensitivitas model terhadap data training\n",
        "**Noise**: Error inherent dalam data\n",
        "\n",
        "#### 1.5.4 Overfitting dan Underfitting\n",
        "\n",
        "##### Overfitting\n",
        "**Definisi**: Model terlalu kompleks, mempelajari noise dalam data training\n",
        "\n",
        "**Tanda-tanda**:\n",
        "- Training error rendah, validation error tinggi\n",
        "- Gap besar antara training dan validation performance\n",
        "- Model terlalu sensitif terhadap data training\n",
        "\n",
        "**Solusi**:\n",
        "- Regularization (L1, L2)\n",
        "- Early stopping\n",
        "- Dropout (untuk neural networks)\n",
        "- Data augmentation\n",
        "- Cross-validation\n",
        "\n",
        "##### Underfitting\n",
        "**Definisi**: Model terlalu sederhana, tidak dapat mempelajari pola dalam data\n",
        "\n",
        "**Tanda-tanda**:\n",
        "- Training error tinggi\n",
        "- Validation error juga tinggi\n",
        "- Model tidak dapat menangkap hubungan dalam data\n",
        "\n",
        "**Solusi**:\n",
        "- Meningkatkan model complexity\n",
        "- Feature engineering\n",
        "- Mengurangi regularization\n",
        "- Menggunakan model yang lebih powerful\n",
        "\n",
        "### 1.6 Statistical Validation dalam ML\n",
        "\n",
        "#### 1.6.1 Cross-Validation\n",
        "**Tujuan**: Mengestimasi performance model pada data yang belum pernah dilihat\n",
        "\n",
        "**Metode**:\n",
        "- **k-Fold CV**: Data dibagi menjadi k subset\n",
        "- **Leave-One-Out CV**: Setiap observasi sebagai test set\n",
        "- **Stratified CV**: Mempertahankan proporsi kelas\n",
        "- **Time Series CV**: Validasi untuk data time series\n",
        "\n",
        "#### 1.6.2 Bootstrap\n",
        "**Tujuan**: Mengestimasi distribusi sampling dan confidence intervals\n",
        "\n",
        "**Metode**:\n",
        "- **Bootstrap Sampling**: Sampling dengan replacement\n",
        "- **Bootstrap Confidence Intervals**: CI untuk parameter\n",
        "- **Bootstrap Model Selection**: Seleksi model dengan bootstrap\n",
        "- **Bagging**: Bootstrap aggregating untuk ensemble\n",
        "\n",
        "#### 1.6.3 Statistical Significance Testing\n",
        "**Tujuan**: Menguji signifikansi statistik dalam model comparison\n",
        "\n",
        "**Metode**:\n",
        "- **t-test**: Perbandingan dua model\n",
        "- **ANOVA**: Perbandingan multiple models\n",
        "- **McNemar's Test**: Perbandingan binary classifiers\n",
        "- **Friedman Test**: Perbandingan multiple models dengan ranking\n",
        "\n",
        "### 1.7 Aplikasi Praktis\n",
        "\n",
        "#### 1.7.1 Medical Diagnosis\n",
        "- **Prediksi Penyakit**: Model untuk mendiagnosis penyakit\n",
        "- **Drug Discovery**: Identifikasi senyawa obat baru\n",
        "- **Medical Imaging**: Analisis gambar medis\n",
        "- **Clinical Trials**: Desain dan analisis uji klinis\n",
        "\n",
        "#### 1.7.2 Financial Modeling\n",
        "- **Risk Assessment**: Penilaian risiko kredit\n",
        "- **Algorithmic Trading**: Trading otomatis\n",
        "- **Fraud Detection**: Deteksi penipuan\n",
        "- **Portfolio Optimization**: Optimasi portofolio\n",
        "\n",
        "#### 1.7.3 Marketing Analytics\n",
        "- **Customer Segmentation**: Segmentasi pelanggan\n",
        "- **Recommendation Systems**: Sistem rekomendasi\n",
        "- **Churn Prediction**: Prediksi customer churn\n",
        "- **A/B Testing**: Pengujian eksperimen\n",
        "\n",
        "#### 1.7.4 Scientific Research\n",
        "- **Drug Discovery**: Penemuan obat baru\n",
        "- **Climate Modeling**: Pemodelan iklim\n",
        "- **Genomics**: Analisis data genomik\n",
        "- **Particle Physics**: Analisis data partikel\n",
        "\n",
        "### 1.8 Best Practices\n",
        "\n",
        "#### 1.8.1 Data Preparation\n",
        "- **Data Quality**: Pastikan data berkualitas tinggi\n",
        "- **Missing Values**: Handle missing values dengan tepat\n",
        "- **Outliers**: Identifikasi dan handle outliers\n",
        "- **Feature Engineering**: Buat fitur yang meaningful\n",
        "\n",
        "#### 1.8.2 Model Selection\n",
        "- **Start Simple**: Mulai dengan model sederhana\n",
        "- **Cross-Validation**: Gunakan CV untuk evaluasi\n",
        "- **Regularization**: Gunakan regularisasi untuk mencegah overfitting\n",
        "- **Ensemble Methods**: Pertimbangkan ensemble methods\n",
        "\n",
        "#### 1.8.3 Validation\n",
        "- **Holdout Validation**: Pisahkan data test\n",
        "- **Cross-Validation**: Gunakan CV untuk model selection\n",
        "- **Statistical Testing**: Uji signifikansi statistik\n",
        "- **Confidence Intervals**: Berikan CI untuk prediksi\n",
        "\n",
        "#### 1.8.4 Interpretation\n",
        "- **Feature Importance**: Analisis pentingnya fitur\n",
        "- **Model Diagnostics**: Periksa asumsi model\n",
        "- **Sensitivity Analysis**: Analisis sensitivitas\n",
        "- **Uncertainty Quantification**: Kuantifikasi ketidakpastian\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.9 Demonstrasi Kode: Pengantar Machine Learning Statistik\n",
        "\n",
        "print(\"=== DEMONSTRASI PENGANTAR MACHINE LEARNING STATISTIK ===\\n\")\n",
        "\n",
        "# 1. Membuat Data Simulasi\n",
        "print(\"1. Membuat Data Simulasi\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Set random seed untuk reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Membuat data dengan hubungan non-linear\n",
        "n_samples = 200\n",
        "X = np.random.uniform(-3, 3, n_samples).reshape(-1, 1)\n",
        "y = 0.5 * X.flatten()**3 - 2 * X.flatten()**2 + X.flatten() + np.random.normal(0, 0.5, n_samples)\n",
        "\n",
        "print(f\"Jumlah sampel: {n_samples}\")\n",
        "print(f\"Shape X: {X.shape}\")\n",
        "print(f\"Shape y: {y.shape}\")\n",
        "print(f\"Range X: [{X.min():.2f}, {X.max():.2f}]\")\n",
        "print(f\"Range y: [{y.min():.2f}, {y.max():.2f}]\")\n",
        "\n",
        "# 2. Split Data\n",
        "print(\"\\n2. Split Data\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Training ratio: {X_train.shape[0]/n_samples:.2f}\")\n",
        "print(f\"Test ratio: {X_test.shape[0]/n_samples:.2f}\")\n",
        "\n",
        "# 3. Model Comparison: Parametric vs Non-Parametric\n",
        "print(\"\\n3. Model Comparison: Parametric vs Non-Parametric\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Parametric: Linear Regression\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "y_pred_linear = linear_model.predict(X_test)\n",
        "\n",
        "# Non-Parametric: Random Forest\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluasi model\n",
        "linear_mse = mean_squared_error(y_test, y_pred_linear)\n",
        "rf_mse = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "linear_r2 = r2_score(y_test, y_pred_linear)\n",
        "rf_r2 = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Linear Regression (Parametric):\")\n",
        "print(f\"  MSE: {linear_mse:.4f}\")\n",
        "print(f\"  R²: {linear_r2:.4f}\")\n",
        "\n",
        "print(\"\\nRandom Forest (Non-Parametric):\")\n",
        "print(f\"  MSE: {rf_mse:.4f}\")\n",
        "print(f\"  R²: {rf_r2:.4f}\")\n",
        "\n",
        "# 4. Cross-Validation\n",
        "print(\"\\n4. Cross-Validation\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Cross-validation untuk Linear Regression\n",
        "linear_scores = cross_val_score(linear_model, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "linear_cv_mse = -linear_scores.mean()\n",
        "linear_cv_std = linear_scores.std()\n",
        "\n",
        "# Cross-validation untuk Random Forest\n",
        "rf_scores = cross_val_score(rf_model, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "rf_cv_mse = -rf_scores.mean()\n",
        "rf_cv_std = rf_scores.std()\n",
        "\n",
        "print(\"K-Fold Cross-Validation (5 folds):\")\n",
        "print(f\"Linear Regression:\")\n",
        "print(f\"  CV MSE: {linear_cv_mse:.4f} ± {linear_cv_std:.4f}\")\n",
        "\n",
        "print(f\"\\nRandom Forest:\")\n",
        "print(f\"  CV MSE: {rf_cv_mse:.4f} ± {rf_cv_std:.4f}\")\n",
        "\n",
        "# 5. Bootstrap Confidence Intervals\n",
        "print(\"\\n5. Bootstrap Confidence Intervals\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Bootstrap untuk R² score\n",
        "def r2_bootstrap(X, y, model, n_bootstrap=1000):\n",
        "    r2_scores = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        # Bootstrap sample\n",
        "        indices = np.random.choice(len(X), len(X), replace=True)\n",
        "        X_boot = X[indices]\n",
        "        y_boot = y[indices]\n",
        "        \n",
        "        # Fit model dan predict\n",
        "        model.fit(X_boot, y_boot)\n",
        "        y_pred = model.predict(X_test)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        r2_scores.append(r2)\n",
        "    \n",
        "    return np.array(r2_scores)\n",
        "\n",
        "# Bootstrap untuk Random Forest\n",
        "rf_r2_bootstrap = r2_bootstrap(X_train, y_train, RandomForestRegressor(n_estimators=50, random_state=42))\n",
        "\n",
        "# Hitung confidence intervals\n",
        "rf_r2_ci = np.percentile(rf_r2_bootstrap, [2.5, 97.5])\n",
        "rf_r2_mean = np.mean(rf_r2_bootstrap)\n",
        "\n",
        "print(f\"Random Forest R² Bootstrap (1000 iterations):\")\n",
        "print(f\"  Mean R²: {rf_r2_mean:.4f}\")\n",
        "print(f\"  95% CI: [{rf_r2_ci[0]:.4f}, {rf_r2_ci[1]:.4f}]\")\n",
        "\n",
        "# 6. Model Complexity Analysis\n",
        "print(\"\\n6. Model Complexity Analysis\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Analisis kompleksitas dengan polynomial features\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for degree in degrees:\n",
        "    # Polynomial regression\n",
        "    poly_model = Pipeline([\n",
        "        ('poly', PolynomialFeatures(degree=degree)),\n",
        "        ('linear', LinearRegression())\n",
        "    ])\n",
        "    \n",
        "    # Fit dan predict\n",
        "    poly_model.fit(X_train, y_train)\n",
        "    \n",
        "    # Training score\n",
        "    y_train_pred = poly_model.predict(X_train)\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    train_scores.append(train_mse)\n",
        "    \n",
        "    # Test score\n",
        "    y_test_pred = poly_model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "    test_scores.append(test_mse)\n",
        "\n",
        "print(\"Polynomial Regression Complexity Analysis:\")\n",
        "print(\"Degree | Train MSE | Test MSE  | Gap\")\n",
        "print(\"-\" * 40)\n",
        "for i, degree in enumerate(degrees):\n",
        "    gap = test_scores[i] - train_scores[i]\n",
        "    print(f\"{degree:6d} | {train_scores[i]:8.4f} | {test_scores[i]:8.4f} | {gap:6.4f}\")\n",
        "\n",
        "# 7. Visualisasi\n",
        "print(\"\\n7. Visualisasi\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Plot 1: Data dan Model Fits\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.scatter(X, y, alpha=0.6, label='Data')\n",
        "X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y_plot_linear = linear_model.predict(X_plot)\n",
        "y_plot_rf = rf_model.predict(X_plot)\n",
        "plt.plot(X_plot, y_plot_linear, 'r-', label='Linear Regression', linewidth=2)\n",
        "plt.plot(X_plot, y_plot_rf, 'g-', label='Random Forest', linewidth=2)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Model Fits')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Residuals\n",
        "plt.subplot(2, 3, 2)\n",
        "residuals_linear = y_test - y_pred_linear\n",
        "residuals_rf = y_test - y_pred_rf\n",
        "plt.scatter(y_pred_linear, residuals_linear, alpha=0.6, label='Linear Regression')\n",
        "plt.scatter(y_pred_rf, residuals_rf, alpha=0.6, label='Random Forest')\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 3: Cross-Validation Scores\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.boxplot([linear_scores, rf_scores], labels=['Linear', 'Random Forest'])\n",
        "plt.ylabel('CV Score (MSE)')\n",
        "plt.title('Cross-Validation Scores')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 4: Bootstrap Distribution\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.hist(rf_r2_bootstrap, bins=30, alpha=0.7, edgecolor='black')\n",
        "plt.axvline(rf_r2_mean, color='red', linestyle='--', label=f'Mean: {rf_r2_mean:.4f}')\n",
        "plt.axvline(rf_r2_ci[0], color='orange', linestyle='--', label=f'95% CI: [{rf_r2_ci[0]:.4f}, {rf_r2_ci[1]:.4f}]')\n",
        "plt.axvline(rf_r2_ci[1], color='orange', linestyle='--')\n",
        "plt.xlabel('R² Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Bootstrap R² Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 5: Model Complexity\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.plot(degrees, train_scores, 'o-', label='Training MSE', linewidth=2)\n",
        "plt.plot(degrees, test_scores, 's-', label='Test MSE', linewidth=2)\n",
        "plt.xlabel('Polynomial Degree')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Model Complexity vs Performance')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 6: Feature Importance (Random Forest)\n",
        "plt.subplot(2, 3, 6)\n",
        "feature_importance = rf_model.feature_importances_\n",
        "plt.bar(range(len(feature_importance)), feature_importance)\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Feature Importance (Random Forest)')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 8. Statistical Tests\n",
        "print(\"\\n8. Statistical Tests\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# t-test untuk perbandingan model\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# Perbandingan MSE\n",
        "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Hitung MSE untuk setiap fold\n",
        "linear_mse_folds = []\n",
        "rf_mse_folds = []\n",
        "\n",
        "for train_idx, val_idx in kfold.split(X):\n",
        "    X_fold_train, X_fold_val = X[train_idx], X[val_idx]\n",
        "    y_fold_train, y_fold_val = y[train_idx], y[val_idx]\n",
        "    \n",
        "    # Linear model\n",
        "    linear_model.fit(X_fold_train, y_fold_train)\n",
        "    y_pred_linear_fold = linear_model.predict(X_fold_val)\n",
        "    linear_mse_folds.append(mean_squared_error(y_fold_val, y_pred_linear_fold))\n",
        "    \n",
        "    # Random Forest model\n",
        "    rf_model.fit(X_fold_train, y_fold_train)\n",
        "    y_pred_rf_fold = rf_model.predict(X_fold_val)\n",
        "    rf_mse_folds.append(mean_squared_error(y_fold_val, y_pred_rf_fold))\n",
        "\n",
        "# Paired t-test\n",
        "t_stat, p_value = ttest_rel(linear_mse_folds, rf_mse_folds)\n",
        "\n",
        "print(\"Paired t-test untuk perbandingan model:\")\n",
        "print(f\"  t-statistic: {t_stat:.4f}\")\n",
        "print(f\"  p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"  → Ada perbedaan signifikan antara model (p < 0.05)\")\n",
        "else:\n",
        "    print(\"  → Tidak ada perbedaan signifikan antara model (p ≥ 0.05)\")\n",
        "\n",
        "# 9. Summary\n",
        "print(\"\\n9. Summary\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"Hasil Analisis Machine Learning Statistik:\")\n",
        "print(f\"1. Data: {n_samples} samples, 1 feature\")\n",
        "print(f\"2. Split: {X_train.shape[0]} training, {X_test.shape[0]} test\")\n",
        "print(f\"3. Linear Regression: MSE = {linear_mse:.4f}, R² = {linear_r2:.4f}\")\n",
        "print(f\"4. Random Forest: MSE = {rf_mse:.4f}, R² = {rf_r2:.4f}\")\n",
        "print(f\"5. Cross-Validation: Linear MSE = {linear_cv_mse:.4f} ± {linear_cv_std:.4f}\")\n",
        "print(f\"6. Bootstrap CI: Random Forest R² = {rf_r2_mean:.4f} [{rf_r2_ci[0]:.4f}, {rf_r2_ci[1]:.4f}]\")\n",
        "print(f\"7. Statistical Test: t = {t_stat:.4f}, p = {p_value:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DEMONSTRASI SELESAI\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Bias-Variance Tradeoff\n",
        "\n",
        "### 2.1 Konsep Dasar Bias dan Variance\n",
        "\n",
        "**Bias-Variance Tradeoff** adalah konsep fundamental dalam machine learning yang menjelaskan hubungan antara bias, variance, dan total error dalam model prediktif.\n",
        "\n",
        "#### 2.1.1 Definisi Bias\n",
        "**Bias** adalah error yang terjadi karena asumsi model yang terlalu sederhana untuk menangkap hubungan yang sebenarnya dalam data.\n",
        "\n",
        "**Karakteristik Bias:**\n",
        "- **High Bias**: Model terlalu sederhana, tidak dapat mempelajari pola kompleks\n",
        "- **Low Bias**: Model cukup fleksibel untuk mempelajari pola dalam data\n",
        "- **Bias Error**: Error yang konsisten dalam prediksi\n",
        "\n",
        "**Contoh High Bias:**\n",
        "- Linear regression untuk data non-linear\n",
        "- Model yang terlalu simple untuk data kompleks\n",
        "- Asumsi yang terlalu kaku\n",
        "\n",
        "#### 2.1.2 Definisi Variance\n",
        "**Variance** adalah error yang terjadi karena sensitivitas model terhadap variasi kecil dalam data training.\n",
        "\n",
        "**Karakteristik Variance:**\n",
        "- **High Variance**: Model terlalu sensitif terhadap data training\n",
        "- **Low Variance**: Model stabil terhadap perubahan data training\n",
        "- **Variance Error**: Error yang bervariasi dengan data training yang berbeda\n",
        "\n",
        "**Contoh High Variance:**\n",
        "- Model yang terlalu kompleks\n",
        "- Overfitting pada data training\n",
        "- Sensitif terhadap noise dalam data\n",
        "\n",
        "#### 2.1.3 Noise\n",
        "**Noise** adalah error inherent dalam data yang tidak dapat dihindari oleh model apapun.\n",
        "\n",
        "**Sumber Noise:**\n",
        "- Measurement error\n",
        "- Sampling error\n",
        "- Random variation\n",
        "- Data quality issues\n",
        "\n",
        "### 2.2 Bias-Variance Decomposition\n",
        "\n",
        "#### 2.2.1 Mathematical Formulation\n",
        "```\n",
        "Expected Error = Bias² + Variance + Noise\n",
        "```\n",
        "\n",
        "Dimana:\n",
        "- **Expected Error**: Error yang diharapkan pada data baru\n",
        "- **Bias²**: Kuadrat dari bias\n",
        "- **Variance**: Varians dari prediksi\n",
        "- **Noise**: Error inherent dalam data\n",
        "\n",
        "#### 2.2.2 Detailed Decomposition\n",
        "```\n",
        "E[(y - f̂(x))²] = [E[f̂(x)] - f(x)]² + E[(f̂(x) - E[f̂(x)])²] + σ²\n",
        "```\n",
        "\n",
        "Dimana:\n",
        "- y = true value\n",
        "- f̂(x) = predicted value\n",
        "- f(x) = true function\n",
        "- σ² = noise variance\n",
        "\n",
        "#### 2.2.3 Interpretation\n",
        "- **Bias²**: Error karena model tidak dapat menangkap true function\n",
        "- **Variance**: Error karena model bervariasi dengan data training\n",
        "- **Noise**: Error yang tidak dapat dihindari\n",
        "\n",
        "### 2.3 Overfitting dan Underfitting\n",
        "\n",
        "#### 2.3.1 Overfitting (High Variance, Low Bias)\n",
        "**Definisi**: Model terlalu kompleks, mempelajari noise dalam data training\n",
        "\n",
        "**Tanda-tanda Overfitting:**\n",
        "- Training error sangat rendah\n",
        "- Validation error tinggi\n",
        "- Gap besar antara training dan validation performance\n",
        "- Model terlalu sensitif terhadap data training\n",
        "\n",
        "**Penyebab Overfitting:**\n",
        "- Model terlalu kompleks\n",
        "- Data training terlalu sedikit\n",
        "- Noise dalam data training\n",
        "- Tidak ada regularisasi\n",
        "\n",
        "**Solusi Overfitting:**\n",
        "- Regularization (L1, L2)\n",
        "- Early stopping\n",
        "- Dropout (untuk neural networks)\n",
        "- Data augmentation\n",
        "- Cross-validation\n",
        "- Ensemble methods\n",
        "\n",
        "#### 2.3.2 Underfitting (High Bias, Low Variance)\n",
        "**Definisi**: Model terlalu sederhana, tidak dapat mempelajari pola dalam data\n",
        "\n",
        "**Tanda-tanda Underfitting:**\n",
        "- Training error tinggi\n",
        "- Validation error juga tinggi\n",
        "- Model tidak dapat menangkap hubungan dalam data\n",
        "- Performance tidak membaik dengan data lebih banyak\n",
        "\n",
        "**Penyebab Underfitting:**\n",
        "- Model terlalu sederhana\n",
        "- Feature engineering tidak cukup\n",
        "- Regularisasi terlalu kuat\n",
        "- Data tidak representatif\n",
        "\n",
        "**Solusi Underfitting:**\n",
        "- Meningkatkan model complexity\n",
        "- Feature engineering\n",
        "- Mengurangi regularisasi\n",
        "- Menggunakan model yang lebih powerful\n",
        "- Menambah data training\n",
        "\n",
        "#### 2.3.3 Optimal Fitting\n",
        "**Tujuan**: Mencapai keseimbangan optimal antara bias dan variance\n",
        "\n",
        "**Karakteristik Optimal Fitting:**\n",
        "- Training error dan validation error seimbang\n",
        "- Gap kecil antara training dan validation performance\n",
        "- Model dapat generalisasi dengan baik\n",
        "- Performance stabil pada data baru\n",
        "\n",
        "### 2.4 Model Complexity vs Performance\n",
        "\n",
        "#### 2.4.1 Learning Curves\n",
        "**Learning Curves** menunjukkan hubungan antara jumlah data training dan performance model.\n",
        "\n",
        "**Karakteristik Learning Curves:**\n",
        "- **Training Curve**: Menurun dengan data lebih banyak\n",
        "- **Validation Curve**: Meningkat dengan data lebih banyak\n",
        "- **Gap**: Perbedaan antara training dan validation performance\n",
        "\n",
        "#### 2.4.2 Complexity Curves\n",
        "**Complexity Curves** menunjukkan hubungan antara model complexity dan performance.\n",
        "\n",
        "**Karakteristik Complexity Curves:**\n",
        "- **Training Performance**: Meningkat dengan complexity\n",
        "- **Validation Performance**: Meningkat kemudian menurun\n",
        "- **Optimal Point**: Titik optimal complexity\n",
        "\n",
        "#### 2.4.3 Bias-Variance Tradeoff Curve\n",
        "**Tradeoff Curve** menunjukkan hubungan antara bias dan variance.\n",
        "\n",
        "**Karakteristik Tradeoff Curve:**\n",
        "- **High Bias, Low Variance**: Model sederhana\n",
        "- **Low Bias, High Variance**: Model kompleks\n",
        "- **Optimal Point**: Keseimbangan optimal\n",
        "\n",
        "### 2.5 Practical Implications\n",
        "\n",
        "#### 2.5.1 Model Selection\n",
        "**Prinsip**: Pilih model dengan keseimbangan optimal antara bias dan variance\n",
        "\n",
        "**Strategi:**\n",
        "- Mulai dengan model sederhana\n",
        "- Tingkatkan complexity secara bertahap\n",
        "- Gunakan cross-validation untuk evaluasi\n",
        "- Pertimbangkan ensemble methods\n",
        "\n",
        "#### 2.5.2 Regularization\n",
        "**Tujuan**: Mengontrol model complexity untuk mencegah overfitting\n",
        "\n",
        "**Metode Regularisasi:**\n",
        "- **L1 Regularization (Lasso)**: Feature selection\n",
        "- **L2 Regularization (Ridge)**: Parameter shrinkage\n",
        "- **Elastic Net**: Kombinasi L1 dan L2\n",
        "- **Early Stopping**: Menghentikan training sebelum overfitting\n",
        "\n",
        "#### 2.5.3 Cross-Validation\n",
        "**Tujuan**: Mengestimasi true performance model\n",
        "\n",
        "**Metode Cross-Validation:**\n",
        "- **k-Fold CV**: Data dibagi menjadi k subset\n",
        "- **Leave-One-Out CV**: Setiap observasi sebagai test set\n",
        "- **Stratified CV**: Mempertahankan proporsi kelas\n",
        "- **Time Series CV**: Validasi untuk data time series\n",
        "\n",
        "#### 2.5.4 Ensemble Methods\n",
        "**Tujuan**: Mengurangi variance dengan menggabungkan multiple models\n",
        "\n",
        "**Metode Ensemble:**\n",
        "- **Bagging**: Bootstrap aggregating\n",
        "- **Boosting**: Sequential learning\n",
        "- **Stacking**: Meta-learning\n",
        "- **Voting**: Majority voting\n",
        "\n",
        "### 2.6 Mathematical Analysis\n",
        "\n",
        "#### 2.6.1 Bias Analysis\n",
        "**Bias** dapat dianalisis dengan:\n",
        "- **Approximation Error**: Error karena model tidak dapat menangkap true function\n",
        "- **Estimation Error**: Error karena estimasi parameter dari data terbatas\n",
        "- **Model Error**: Error karena asumsi model yang salah\n",
        "\n",
        "#### 2.6.2 Variance Analysis\n",
        "**Variance** dapat dianalisis dengan:\n",
        "- **Sampling Variance**: Variasi karena sampling data training\n",
        "- **Model Variance**: Variasi karena model complexity\n",
        "- **Parameter Variance**: Variasi karena estimasi parameter\n",
        "\n",
        "#### 2.6.3 Tradeoff Analysis\n",
        "**Tradeoff** dapat dianalisis dengan:\n",
        "- **Bias-Variance Decomposition**: Decomposisi error menjadi bias dan variance\n",
        "- **Learning Curves**: Analisis performance vs data size\n",
        "- **Complexity Curves**: Analisis performance vs model complexity\n",
        "\n",
        "### 2.7 Aplikasi Praktis\n",
        "\n",
        "#### 2.7.1 Model Selection\n",
        "- **Linear Models**: Low variance, high bias\n",
        "- **Tree Models**: High variance, low bias\n",
        "- **Ensemble Models**: Balanced bias-variance\n",
        "- **Neural Networks**: Adjustable bias-variance\n",
        "\n",
        "#### 2.7.2 Hyperparameter Tuning\n",
        "- **Learning Rate**: Mempengaruhi convergence dan stability\n",
        "- **Regularization Parameter**: Mempengaruhi bias-variance tradeoff\n",
        "- **Model Complexity**: Mempengaruhi capacity model\n",
        "- **Ensemble Size**: Mempengaruhi variance reduction\n",
        "\n",
        "#### 2.7.3 Feature Engineering\n",
        "- **Feature Selection**: Mengurangi variance\n",
        "- **Feature Creation**: Mengurangi bias\n",
        "- **Dimensionality Reduction**: Mengurangi variance\n",
        "- **Feature Scaling**: Mempengaruhi convergence\n",
        "\n",
        "### 2.8 Best Practices\n",
        "\n",
        "#### 2.8.1 Model Development\n",
        "- **Start Simple**: Mulai dengan model sederhana\n",
        "- **Iterative Improvement**: Tingkatkan complexity secara bertahap\n",
        "- **Cross-Validation**: Gunakan CV untuk evaluasi\n",
        "- **Regularization**: Gunakan regularisasi untuk kontrol complexity\n",
        "\n",
        "#### 2.8.2 Evaluation\n",
        "- **Multiple Metrics**: Gunakan multiple evaluation metrics\n",
        "- **Learning Curves**: Analisis learning curves\n",
        "- **Validation Curves**: Analisis validation curves\n",
        "- **Statistical Testing**: Uji signifikansi perbedaan model\n",
        "\n",
        "#### 2.8.3 Interpretation\n",
        "- **Bias Analysis**: Analisis sumber bias\n",
        "- **Variance Analysis**: Analisis sumber variance\n",
        "- **Tradeoff Analysis**: Analisis tradeoff optimal\n",
        "- **Practical Implications**: Implikasi praktis untuk aplikasi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.9 Demonstrasi Kode: Bias-Variance Tradeoff\n",
        "\n",
        "print(\"=== DEMONSTRASI BIAS-VARIANCE TRADEOFF ===\\n\")\n",
        "\n",
        "# 1. Membuat Data Simulasi\n",
        "print(\"1. Membuat Data Simulasi\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Set random seed untuk reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# True function: f(x) = 0.5x³ - 2x² + x + noise\n",
        "def true_function(x):\n",
        "    return 0.5 * x**3 - 2 * x**2 + x\n",
        "\n",
        "# Generate data\n",
        "n_samples = 1000\n",
        "X = np.random.uniform(-3, 3, n_samples)\n",
        "y = true_function(X) + np.random.normal(0, 0.5, n_samples)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Jumlah sampel: {n_samples}\")\n",
        "print(f\"Training set: {len(X_train)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "\n",
        "# 2. Model dengan Berbagai Kompleksitas\n",
        "print(\"\\n2. Model dengan Berbagai Kompleksitas\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Model dengan berbagai degree polynomial\n",
        "degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "models = {}\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for degree in degrees:\n",
        "    # Polynomial regression\n",
        "    model = Pipeline([\n",
        "        ('poly', PolynomialFeatures(degree=degree)),\n",
        "        ('linear', LinearRegression())\n",
        "    ])\n",
        "    \n",
        "    # Fit model\n",
        "    model.fit(X_train.reshape(-1, 1), y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train.reshape(-1, 1))\n",
        "    y_test_pred = model.predict(X_test.reshape(-1, 1))\n",
        "    \n",
        "    # Scores\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "    \n",
        "    models[degree] = model\n",
        "    train_scores.append(train_mse)\n",
        "    test_scores.append(test_mse)\n",
        "    \n",
        "    print(f\"Degree {degree:2d}: Train MSE = {train_mse:.4f}, Test MSE = {test_mse:.4f}\")\n",
        "\n",
        "# 3. Bias-Variance Analysis\n",
        "print(\"\\n3. Bias-Variance Analysis\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Hitung bias dan variance untuk setiap model\n",
        "def calculate_bias_variance(model, X_test, y_test, n_bootstrap=100):\n",
        "    \"\"\"Hitung bias dan variance untuk model\"\"\"\n",
        "    predictions = []\n",
        "    \n",
        "    # Bootstrap sampling\n",
        "    for _ in range(n_bootstrap):\n",
        "        # Bootstrap sample\n",
        "        indices = np.random.choice(len(X_train), len(X_train), replace=True)\n",
        "        X_boot = X_train[indices].reshape(-1, 1)\n",
        "        y_boot = y_train[indices]\n",
        "        \n",
        "        # Fit model pada bootstrap sample\n",
        "        model.fit(X_boot, y_boot)\n",
        "        \n",
        "        # Predict pada test set\n",
        "        y_pred = model.predict(X_test.reshape(-1, 1))\n",
        "        predictions.append(y_pred)\n",
        "    \n",
        "    predictions = np.array(predictions)\n",
        "    \n",
        "    # Hitung bias dan variance\n",
        "    mean_pred = np.mean(predictions, axis=0)\n",
        "    bias_squared = np.mean((mean_pred - y_test)**2)\n",
        "    variance = np.mean(np.var(predictions, axis=0))\n",
        "    \n",
        "    return bias_squared, variance\n",
        "\n",
        "# Analisis untuk beberapa model\n",
        "selected_degrees = [1, 3, 5, 7, 9]\n",
        "bias_scores = []\n",
        "variance_scores = []\n",
        "total_scores = []\n",
        "\n",
        "print(\"Bias-Variance Analysis:\")\n",
        "print(\"Degree | Bias²    | Variance | Total   | Noise\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for degree in selected_degrees:\n",
        "    model = models[degree]\n",
        "    bias_sq, variance = calculate_bias_variance(model, X_test, y_test)\n",
        "    \n",
        "    # Total error = bias² + variance + noise\n",
        "    # Noise diestimasi sebagai MSE minimum yang mungkin\n",
        "    noise = 0.25  # Variance dari noise yang ditambahkan\n",
        "    total_error = bias_sq + variance + noise\n",
        "    \n",
        "    bias_scores.append(bias_sq)\n",
        "    variance_scores.append(variance)\n",
        "    total_scores.append(total_error)\n",
        "    \n",
        "    print(f\"{degree:6d} | {bias_sq:8.4f} | {variance:8.4f} | {total_error:7.4f} | {noise:6.4f}\")\n",
        "\n",
        "# 4. Learning Curves\n",
        "print(\"\\n4. Learning Curves\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Hitung learning curves untuk model dengan degree 3 dan 7\n",
        "def learning_curve(model, X, y, train_sizes):\n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "    \n",
        "    for size in train_sizes:\n",
        "        # Sample data\n",
        "        indices = np.random.choice(len(X), size, replace=False)\n",
        "        X_sample = X[indices].reshape(-1, 1)\n",
        "        y_sample = y[indices]\n",
        "        \n",
        "        # Split untuk validation\n",
        "        X_train_sample, X_val_sample, y_train_sample, y_val_sample = train_test_split(\n",
        "            X_sample, y_sample, test_size=0.3, random_state=42\n",
        "        )\n",
        "        \n",
        "        # Fit model\n",
        "        model.fit(X_train_sample, y_train_sample)\n",
        "        \n",
        "        # Scores\n",
        "        train_score = model.score(X_train_sample, y_train_sample)\n",
        "        val_score = model.score(X_val_sample, y_val_sample)\n",
        "        \n",
        "        train_scores.append(train_score)\n",
        "        val_scores.append(val_score)\n",
        "    \n",
        "    return train_scores, val_scores\n",
        "\n",
        "# Learning curves\n",
        "train_sizes = [20, 50, 100, 200, 300, 400, 500, 600, 700]\n",
        "\n",
        "# Model degree 3 (underfitting)\n",
        "model_underfit = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=3)),\n",
        "    ('linear', LinearRegression())\n",
        "])\n",
        "train_scores_3, val_scores_3 = learning_curve(model_underfit, X_train, y_train, train_sizes)\n",
        "\n",
        "# Model degree 7 (overfitting)\n",
        "model_overfit = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=7)),\n",
        "    ('linear', LinearRegression())\n",
        "])\n",
        "train_scores_7, val_scores_7 = learning_curve(model_overfit, X_train, y_train, train_sizes)\n",
        "\n",
        "print(\"Learning Curves (R² Score):\")\n",
        "print(\"Size | Degree 3 Train | Degree 3 Val | Degree 7 Train | Degree 7 Val\")\n",
        "print(\"-\" * 70)\n",
        "for i, size in enumerate(train_sizes):\n",
        "    print(f\"{size:4d} | {train_scores_3[i]:12.4f} | {val_scores_3[i]:11.4f} | {train_scores_7[i]:13.4f} | {val_scores_7[i]:10.4f}\")\n",
        "\n",
        "# 5. Visualisasi\n",
        "print(\"\\n5. Visualisasi\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Plot 1: Model Fits\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "plt.subplot(3, 3, 1)\n",
        "X_plot = np.linspace(-3, 3, 100)\n",
        "y_true = true_function(X_plot)\n",
        "plt.scatter(X, y, alpha=0.3, label='Data', s=10)\n",
        "plt.plot(X_plot, y_true, 'k-', label='True Function', linewidth=2)\n",
        "\n",
        "# Plot beberapa model\n",
        "for degree in [1, 3, 5, 7, 9]:\n",
        "    model = models[degree]\n",
        "    y_pred = model.predict(X_plot.reshape(-1, 1))\n",
        "    plt.plot(X_plot, y_pred, label=f'Degree {degree}', linewidth=2)\n",
        "\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Model Fits dengan Berbagai Kompleksitas')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Bias-Variance Tradeoff\n",
        "plt.subplot(3, 3, 2)\n",
        "plt.plot(selected_degrees, bias_scores, 'o-', label='Bias²', linewidth=2, markersize=8)\n",
        "plt.plot(selected_degrees, variance_scores, 's-', label='Variance', linewidth=2, markersize=8)\n",
        "plt.plot(selected_degrees, total_scores, '^-', label='Total Error', linewidth=2, markersize=8)\n",
        "plt.xlabel('Polynomial Degree')\n",
        "plt.ylabel('Error')\n",
        "plt.title('Bias-Variance Tradeoff')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 3: Learning Curves\n",
        "plt.subplot(3, 3, 3)\n",
        "plt.plot(train_sizes, train_scores_3, 'o-', label='Degree 3 Train', linewidth=2)\n",
        "plt.plot(train_sizes, val_scores_3, 's-', label='Degree 3 Val', linewidth=2)\n",
        "plt.plot(train_sizes, train_scores_7, 'o-', label='Degree 7 Train', linewidth=2)\n",
        "plt.plot(train_sizes, val_scores_7, 's-', label='Degree 7 Val', linewidth=2)\n",
        "plt.xlabel('Training Size')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Learning Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 4: Training vs Test Error\n",
        "plt.subplot(3, 3, 4)\n",
        "plt.plot(degrees, train_scores, 'o-', label='Training Error', linewidth=2)\n",
        "plt.plot(degrees, test_scores, 's-', label='Test Error', linewidth=2)\n",
        "plt.xlabel('Polynomial Degree')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Training vs Test Error')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 5: Error Gap\n",
        "plt.subplot(3, 3, 5)\n",
        "error_gap = np.array(test_scores) - np.array(train_scores)\n",
        "plt.plot(degrees, error_gap, 'o-', label='Error Gap', linewidth=2, color='red')\n",
        "plt.xlabel('Polynomial Degree')\n",
        "plt.ylabel('Error Gap')\n",
        "plt.title('Overfitting Indicator')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 6: Model Complexity vs Performance\n",
        "plt.subplot(3, 3, 6)\n",
        "plt.plot(degrees, train_scores, 'o-', label='Training', linewidth=2)\n",
        "plt.plot(degrees, test_scores, 's-', label='Test', linewidth=2)\n",
        "plt.axvline(x=3, color='green', linestyle='--', label='Optimal', linewidth=2)\n",
        "plt.xlabel('Polynomial Degree')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Model Complexity vs Performance')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 7: Residuals Analysis (Degree 3)\n",
        "plt.subplot(3, 3, 7)\n",
        "model_3 = models[3]\n",
        "y_pred_3 = model_3.predict(X_test.reshape(-1, 1))\n",
        "residuals_3 = y_test - y_pred_3\n",
        "plt.scatter(y_pred_3, residuals_3, alpha=0.6)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals (Degree 3)')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 8: Residuals Analysis (Degree 7)\n",
        "plt.subplot(3, 3, 8)\n",
        "model_7 = models[7]\n",
        "y_pred_7 = model_7.predict(X_test.reshape(-1, 1))\n",
        "residuals_7 = y_test - y_pred_7\n",
        "plt.scatter(y_pred_7, residuals_7, alpha=0.6)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals (Degree 7)')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 9: Optimal Model\n",
        "plt.subplot(3, 3, 9)\n",
        "optimal_degree = 3\n",
        "optimal_model = models[optimal_degree]\n",
        "y_pred_optimal = optimal_model.predict(X_plot.reshape(-1, 1))\n",
        "plt.scatter(X, y, alpha=0.3, label='Data', s=10)\n",
        "plt.plot(X_plot, y_true, 'k-', label='True Function', linewidth=2)\n",
        "plt.plot(X_plot, y_pred_optimal, 'r-', label=f'Optimal Model (Degree {optimal_degree})', linewidth=2)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Optimal Model')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Statistical Analysis\n",
        "print(\"\\n6. Statistical Analysis\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# T-test untuk perbandingan model\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# Perbandingan degree 3 vs degree 7\n",
        "model_3_scores = []\n",
        "model_7_scores = []\n",
        "\n",
        "# Cross-validation scores\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for train_idx, val_idx in kfold.split(X_train):\n",
        "    X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
        "    y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
        "    \n",
        "    # Model degree 3\n",
        "    model_3.fit(X_fold_train.reshape(-1, 1), y_fold_train)\n",
        "    y_pred_3_fold = model_3.predict(X_fold_val.reshape(-1, 1))\n",
        "    mse_3 = mean_squared_error(y_fold_val, y_pred_3_fold)\n",
        "    model_3_scores.append(mse_3)\n",
        "    \n",
        "    # Model degree 7\n",
        "    model_7.fit(X_fold_train.reshape(-1, 1), y_fold_train)\n",
        "    y_pred_7_fold = model_7.predict(X_fold_val.reshape(-1, 1))\n",
        "    mse_7 = mean_squared_error(y_fold_val, y_pred_7_fold)\n",
        "    model_7_scores.append(mse_7)\n",
        "\n",
        "# Paired t-test\n",
        "t_stat, p_value = ttest_rel(model_3_scores, model_7_scores)\n",
        "\n",
        "print(\"Paired t-test (Degree 3 vs Degree 7):\")\n",
        "print(f\"  Degree 3 MSE: {np.mean(model_3_scores):.4f} ± {np.std(model_3_scores):.4f}\")\n",
        "print(f\"  Degree 7 MSE: {np.mean(model_7_scores):.4f} ± {np.std(model_7_scores):.4f}\")\n",
        "print(f\"  t-statistic: {t_stat:.4f}\")\n",
        "print(f\"  p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"  → Ada perbedaan signifikan antara model (p < 0.05)\")\n",
        "else:\n",
        "    print(\"  → Tidak ada perbedaan signifikan antara model (p ≥ 0.05)\")\n",
        "\n",
        "# 7. Summary\n",
        "print(\"\\n7. Summary\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Temukan model optimal\n",
        "optimal_idx = np.argmin(test_scores)\n",
        "optimal_degree = degrees[optimal_idx]\n",
        "optimal_mse = test_scores[optimal_idx]\n",
        "\n",
        "print(\"Hasil Analisis Bias-Variance Tradeoff:\")\n",
        "print(f\"1. Model optimal: Degree {optimal_degree}\")\n",
        "print(f\"2. Optimal MSE: {optimal_mse:.4f}\")\n",
        "print(f\"3. Bias² (Degree 3): {bias_scores[1]:.4f}\")\n",
        "print(f\"4. Variance (Degree 3): {variance_scores[1]:.4f}\")\n",
        "print(f\"5. Bias² (Degree 7): {bias_scores[3]:.4f}\")\n",
        "print(f\"6. Variance (Degree 7): {variance_scores[3]:.4f}\")\n",
        "print(f\"7. Error Gap (Degree 3): {test_scores[2] - train_scores[2]:.4f}\")\n",
        "print(f\"8. Error Gap (Degree 7): {test_scores[6] - train_scores[6]:.4f}\")\n",
        "\n",
        "print(\"\\nInterpretasi:\")\n",
        "if optimal_degree <= 3:\n",
        "    print(\"  → Model cenderung underfitting (high bias, low variance)\")\n",
        "elif optimal_degree >= 7:\n",
        "    print(\"  → Model cenderung overfitting (low bias, high variance)\")\n",
        "else:\n",
        "    print(\"  → Model mencapai keseimbangan optimal (balanced bias-variance)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DEMONSTRASI SELESAI\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
